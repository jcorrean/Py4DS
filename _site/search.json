[
  {
    "objectID": "Py4DS.html",
    "href": "Py4DS.html",
    "title": "Analítica de Proyectos",
    "section": "",
    "text": "Hay varias razones de peso para considerar otras opciones más allá de Excel y de Microsoft Project para gestionar proyectos. A continuación algunas de ellas.\n\nLa Ciencia de Datos (Hathaway y Larson 2021) y la Inteligencia Artificial (Russell y Norvig 2022) tienen el potencial de revolucionar la forma cómo trabajamos y resolvemos problemas de gran complejidad.\nLa ciencia de datos y la inteligencia artificial no trabajan con herramientas de escritorio como las ofrecidas por Microsoft Office (i.e., Word, Excel, PowerPoint) debido a las llamadas 4Vs: Veracidad, Variabilidad, Volumen, y Velocidad con la que se producen los datos en el mundo real (Provost y Fawcett 2013).\nExcel es limitado a la hora abrir archivos con datos de gran volumen que están en el orden de los millones de filas o columnas. (Excel no sirve para hacer “Big Data”)\nCon Microsoft Project no podemos saber los pequeños cambios que ocurrieron en cada uno de los archivos asociados al desarrollo del proyecto. Con Python y R, estos cambios pueden gestionarse de mánera ágil usando una potente herramienta llamada GitHub.\nNi Excel ni Project son útiles para analizar “datos no estructurados” tales como colecciones de documentos, fotos, videos, o audios. En cambio los datos no estructurados son analizables con Python y R de una forma más versátil y ágil. Sobre ello hablaremos más en detalle en un próximo blog sobre HuggingFace\nEl análisis de datos no estructurados es la base fundamental de herramientas como el aprendizaje automático (conocido en inglés como Machine Learning).\n\nParte de lo que veremos en las próximas subsecciones está tomado del libro de texto de Hathaway y Larson (2021) quienes se inspiraron en la “biblia” de la ciencia de datos escrita por Wickham y Grolemund (2017).\n\n\nLa extracción, transformación y carga de datos estructurados en inglés se abrevia con las siglas “ETL” (“extraction, transformation and loading”). ETL es un ejercicio muy típico en tareas de big data sobre las cuales se apoyan rutinariamente aplicaciones de inteligencia artificial tales como el aprendizaje automático. La sintaxis que veremos a continuación nos permite abrir un total de 22 archivos separados por comas (.csv) con datos de vuelos internacionales. Parte de lo que veremos acá está adaptado del blog de Marc Garcia (2022). Para ello, nos vamos a apoyar en “pandas” la cual es una librería muy conocida dentro de Python.\nEsta librería fue diseñada para realizar tareas relacionadas con la lectura, edición, y manipulación de datos estructurados. Por datos estructurados vamos a comprender una tabla en donde las variables que nos interesa analizar se ordenan por columnas, las observaciones de cada variable se ordenan por filas, y los valores de cada observación para cada variable se registran en cada celda, tal como aparece en la siguiente imágen.\n\n\n\n\n\nAdemás de pandas, vamos a usar otras librerías como “importlib” y “pyarrow” para acortar el tiempo que nos tomará abrir los siguientes datos.\n\nimport importlib\n\ntry:\n    import pandas\n    import pyarrow\n    import pyarrow.csv\n\n    COLUMN_TYPES = {'Origin': pyarrow.dictionary(pyarrow.int32(), pyarrow.string()),\n                    'Year': pyarrow.uint16(),\n                    'Month': pyarrow.uint8(),\n                    'DayofMonth': pyarrow.uint8(),\n                    'CRSDepTime': pyarrow.uint16(),\n                    'DepTime': pyarrow.uint16()}\n\n    tables = []\n    for year in range(1987, 2009):\n        tables.append(pyarrow.csv.read_csv(\n            f'/home/jcc/dataverse_files/{year}.csv',\n            convert_options=pyarrow.csv.ConvertOptions(\n                include_columns=COLUMN_TYPES,\n                column_types=COLUMN_TYPES)))\n\n    df = pyarrow.concat_tables(tables).to_pandas()\n    \nexcept ImportError:\n    # Si la librería pyarrow no está instalada, primero asegúrese de instalarla\n    print(\"PyArrow no está instalado, Por favor, instala pyarrow escribiendo 'pip install pyarrow'\")\n\nLos datos que hemos abierto, tienen la siguiente apariencia\n\ndf.head(5)\n\n\n\n\n\n\n\n\nOrigin\nYear\nMonth\nDayofMonth\nCRSDepTime\nDepTime\n\n\n\n\n0\nSAN\n1987\n10\n14\n730\n741.0\n\n\n1\nSAN\n1987\n10\n15\n730\n729.0\n\n\n2\nSAN\n1987\n10\n17\n730\n741.0\n\n\n3\nSAN\n1987\n10\n18\n730\n729.0\n\n\n4\nSAN\n1987\n10\n19\n730\n749.0\n\n\n\n\n\n\n\nPodemos hacer algunos pequeños cambios a la base de datos de la siguiente manera\n\ndate = pandas.to_datetime(df[['Year', 'Month', 'DayofMonth']].rename(columns={'DayofMonth': 'Day'}))\n\ndf['scheduled_dep'] = date + pandas.to_timedelta((df['CRSDepTime'] // 100) * 60 + (df['CRSDepTime'] % 100),\n                                                 unit='minutes')\ndf['actual_dep'] = date + pandas.to_timedelta((df['DepTime'] // 100) * 60 + (df['DepTime'] % 100),\n                                            unit='minutes')\n\ndel date\ndf = df[['Origin', 'scheduled_dep', 'actual_dep']]\n\ndf['delay'] = (df['actual_dep'] - df['scheduled_dep']).dt.total_seconds() / 60 / 60\n\ndf['delay'] = df['delay'].where(df['delay'] &gt; - 2, 24 - df['delay'])\ndf.tail(5)\n\n\n\n\n\n\n\n\nOrigin\nscheduled_dep\nactual_dep\ndelay\n\n\n\n\n118914453\nBOS\n2008-04-17 10:25:00\n2008-04-17 10:25:00\n0.000000\n\n\n118914454\nCVG\n2008-04-17 13:20:00\n2008-04-17 13:19:00\n-0.016667\n\n\n118914455\nBOS\n2008-04-17 13:35:00\n2008-04-17 13:35:00\n0.000000\n\n\n118914456\nCVG\n2008-04-17 19:35:00\n2008-04-17 19:33:00\n-0.033333\n\n\n118914457\nBWI\n2008-04-17 06:15:00\n2008-04-17 06:21:00\n0.100000\n\n\n\n\n\n\n\nLuego de esta última sintaxis en Python vemos que la base de datos que hemos abierto tiene un total de 118.914.457 filas y cuatro columnas, aunque originalmente la base de datos tenía un total de seis columnas. A continuación te comparto algunas preguntas sugeridas para que prodfundices en tu aprendizaje sobre la analítica de datos."
  },
  {
    "objectID": "Py4DS.html#organización-de-datos-con-pandas",
    "href": "Py4DS.html#organización-de-datos-con-pandas",
    "title": "Analítica de Proyectos",
    "section": "",
    "text": "Pandas es una librería para Python que está diseñada para realizar tareas relacionadas con la lectura, edición, y manipulación de datos estructurados. Por datos estructurados vamos a comprender una tabla en donde las variables que nos interesa analizar se ordenan por columnas, las observaciones de cada variable se ordenan por filas, y los valores de cada observación para cada variable se registran en cada celda, tal como aparece en la siguiente imágen.\n\n\n\n\n\nLo primero que vamos a hacer es un ejercicio de “ETL” (acrónimo del inglés que se refiere a “extraction, transformation and loading”) que suele ser muy típico en tareas de big data. La sintaxis que veremos a continuación nos permite abrir un total de 22 archivos separados por comas (.csv) con datos de vuelos internacionales. Parte de lo que veremos acá está adaptado del blog de Marc Garcia (2022).\nUna vez realizado lo anterior, ahora pasamos a abrir la data\n\nimport importlib\n\n\ntry:\n    import pandas\n    import pyarrow\n    import pyarrow.csv\n\n    COLUMN_TYPES = {'Origin': pyarrow.dictionary(pyarrow.int32(), pyarrow.string()),\n                    'Year': pyarrow.uint16(),\n                    'Month': pyarrow.uint8(),\n                    'DayofMonth': pyarrow.uint8(),\n                    'CRSDepTime': pyarrow.uint16(),\n                    'DepTime': pyarrow.uint16()}\n\n    tables = []\n    for year in range(1987, 2009):\n        tables.append(pyarrow.csv.read_csv(\n            f'/home/jcc/dataverse_files/{year}.csv',\n            convert_options=pyarrow.csv.ConvertOptions(\n                include_columns=COLUMN_TYPES,\n                column_types=COLUMN_TYPES)))\n\n    df = pyarrow.concat_tables(tables).to_pandas()\n    \nexcept ImportError:\n    # PyArrow is not installed, handle the situation appropriately\n    print(\"PyArrow is not installed. Please install it using 'pip install pyarrow'\")\n    # You might consider alternative approaches like using Pandas or Dask for CSV reading and processing.\n\nLos datos que hemos abierto, tienen la siguiente apariencia\n\ndf.head(5)\n\n\n\n\n\n\n\n\nOrigin\nYear\nMonth\nDayofMonth\nCRSDepTime\nDepTime\n\n\n\n\n0\nSAN\n1987\n10\n14\n730\n741.0\n\n\n1\nSAN\n1987\n10\n15\n730\n729.0\n\n\n2\nSAN\n1987\n10\n17\n730\n741.0\n\n\n3\nSAN\n1987\n10\n18\n730\n729.0\n\n\n4\nSAN\n1987\n10\n19\n730\n749.0\n\n\n\n\n\n\n\nPodemos hacer algunos pequeños cambios a la base de datos de la siguiente manera\n\ndate = pandas.to_datetime(df[['Year', 'Month', 'DayofMonth']].rename(columns={'DayofMonth': 'Day'}))\n\ndf['scheduled_dep'] = date + pandas.to_timedelta((df['CRSDepTime'] // 100) * 60 + (df['CRSDepTime'] % 100),\n                                                 unit='minutes')\ndf['actual_dep'] = date + pandas.to_timedelta((df['DepTime'] // 100) * 60 + (df['DepTime'] % 100),\n                                            unit='minutes')\n\ndel date\ndf = df[['Origin', 'scheduled_dep', 'actual_dep']]\n\ndf['delay'] = (df['actual_dep'] - df['scheduled_dep']).dt.total_seconds() / 60 / 60\n\ndf['delay'] = df['delay'].where(df['delay'] &gt; - 2, 24 - df['delay'])\ndf.tail(5)\n\n\n\n\n\n\n\n\nOrigin\nscheduled_dep\nactual_dep\ndelay\n\n\n\n\n118914453\nBOS\n2008-04-17 10:25:00\n2008-04-17 10:25:00\n0.000000\n\n\n118914454\nCVG\n2008-04-17 13:20:00\n2008-04-17 13:19:00\n-0.016667\n\n\n118914455\nBOS\n2008-04-17 13:35:00\n2008-04-17 13:35:00\n0.000000\n\n\n118914456\nCVG\n2008-04-17 19:35:00\n2008-04-17 19:33:00\n-0.033333\n\n\n118914457\nBWI\n2008-04-17 06:15:00\n2008-04-17 06:21:00\n0.100000\n\n\n\n\n\n\n\nLuego de esta última sintaxis en Python vemos que la base de datos que hemos abierto tiene un total de 118.914.457 filas y cuatro columnas, aunque originalmente la base de datos tenía un total de seis columnas."
  },
  {
    "objectID": "Py4DS.html#extracción-transformación-y-carga-de-datos-estructurados",
    "href": "Py4DS.html#extracción-transformación-y-carga-de-datos-estructurados",
    "title": "Analítica de Proyectos",
    "section": "",
    "text": "La extracción, transformación y carga de datos estructurados en inglés se abrevia con las siglas “ETL” (“extraction, transformation and loading”). ETL es un ejercicio muy típico en tareas de big data sobre las cuales se apoyan rutinariamente aplicaciones de inteligencia artificial tales como el aprendizaje automático. La sintaxis que veremos a continuación nos permite abrir un total de 22 archivos separados por comas (.csv) con datos de vuelos internacionales. Parte de lo que veremos acá está adaptado del blog de Marc Garcia (2022). Para ello, nos vamos a apoyar en “pandas” la cual es una librería muy conocida dentro de Python.\nEsta librería fue diseñada para realizar tareas relacionadas con la lectura, edición, y manipulación de datos estructurados. Por datos estructurados vamos a comprender una tabla en donde las variables que nos interesa analizar se ordenan por columnas, las observaciones de cada variable se ordenan por filas, y los valores de cada observación para cada variable se registran en cada celda, tal como aparece en la siguiente imágen.\n\n\n\n\n\nAdemás de pandas, vamos a usar otras librerías como “importlib” y “pyarrow” para acortar el tiempo que nos tomará abrir los siguientes datos.\n\nimport importlib\n\ntry:\n    import pandas\n    import pyarrow\n    import pyarrow.csv\n\n    COLUMN_TYPES = {'Origin': pyarrow.dictionary(pyarrow.int32(), pyarrow.string()),\n                    'Year': pyarrow.uint16(),\n                    'Month': pyarrow.uint8(),\n                    'DayofMonth': pyarrow.uint8(),\n                    'CRSDepTime': pyarrow.uint16(),\n                    'DepTime': pyarrow.uint16()}\n\n    tables = []\n    for year in range(1987, 2009):\n        tables.append(pyarrow.csv.read_csv(\n            f'/home/jcc/dataverse_files/{year}.csv',\n            convert_options=pyarrow.csv.ConvertOptions(\n                include_columns=COLUMN_TYPES,\n                column_types=COLUMN_TYPES)))\n\n    df = pyarrow.concat_tables(tables).to_pandas()\n    \nexcept ImportError:\n    # Si la librería pyarrow no está instalada, primero asegúrese de instalarla\n    print(\"PyArrow no está instalado, Por favor, instala pyarrow escribiendo 'pip install pyarrow'\")\n\nLos datos que hemos abierto, tienen la siguiente apariencia\n\ndf.head(5)\n\n\n\n\n\n\n\n\nOrigin\nYear\nMonth\nDayofMonth\nCRSDepTime\nDepTime\n\n\n\n\n0\nSAN\n1987\n10\n14\n730\n741.0\n\n\n1\nSAN\n1987\n10\n15\n730\n729.0\n\n\n2\nSAN\n1987\n10\n17\n730\n741.0\n\n\n3\nSAN\n1987\n10\n18\n730\n729.0\n\n\n4\nSAN\n1987\n10\n19\n730\n749.0\n\n\n\n\n\n\n\nPodemos hacer algunos pequeños cambios a la base de datos de la siguiente manera\n\ndate = pandas.to_datetime(df[['Year', 'Month', 'DayofMonth']].rename(columns={'DayofMonth': 'Day'}))\n\ndf['scheduled_dep'] = date + pandas.to_timedelta((df['CRSDepTime'] // 100) * 60 + (df['CRSDepTime'] % 100),\n                                                 unit='minutes')\ndf['actual_dep'] = date + pandas.to_timedelta((df['DepTime'] // 100) * 60 + (df['DepTime'] % 100),\n                                            unit='minutes')\n\ndel date\ndf = df[['Origin', 'scheduled_dep', 'actual_dep']]\n\ndf['delay'] = (df['actual_dep'] - df['scheduled_dep']).dt.total_seconds() / 60 / 60\n\ndf['delay'] = df['delay'].where(df['delay'] &gt; - 2, 24 - df['delay'])\ndf.tail(5)\n\n\n\n\n\n\n\n\nOrigin\nscheduled_dep\nactual_dep\ndelay\n\n\n\n\n118914453\nBOS\n2008-04-17 10:25:00\n2008-04-17 10:25:00\n0.000000\n\n\n118914454\nCVG\n2008-04-17 13:20:00\n2008-04-17 13:19:00\n-0.016667\n\n\n118914455\nBOS\n2008-04-17 13:35:00\n2008-04-17 13:35:00\n0.000000\n\n\n118914456\nCVG\n2008-04-17 19:35:00\n2008-04-17 19:33:00\n-0.033333\n\n\n118914457\nBWI\n2008-04-17 06:15:00\n2008-04-17 06:21:00\n0.100000\n\n\n\n\n\n\n\nLuego de esta última sintaxis en Python vemos que la base de datos que hemos abierto tiene un total de 118.914.457 filas y cuatro columnas, aunque originalmente la base de datos tenía un total de seis columnas. A continuación te comparto algunas preguntas sugeridas para que prodfundices en tu aprendizaje sobre la analítica de datos."
  }
]