---
title: "Analítica de Proyectos"
format: html
keywords:
  - Analítica de datos
  - Python y R
  - Proyectos de Complejidad
  - GitHub
abstract: |
  Nuestro planeta enfrenta problemas de gran complejidad. Por ejemplo, las soluciones al calentamiento global probablemente necesiten la combinación de varias disciplinas profesionales tales como la geofísica, la ingeniería, y la computación. Con estos desafíos, es evidente la importancia de aprender otras herramientas que vayan mucho más allá de lo que hemos aprendido con Microsoft Excel y Microsoft Project. En este blog tendrás un panorama práctico de cómo se integran herramientas de analítica de datos (Python y R) a la gestión y control de proyectos.
---

La Ciencia de Datos [@Hathaway2021] y la Inteligencia Artificial [@Russell2022] tienen el potencial de revolucionar la forma cómo trabajamos y resolvemos problemas de gran complejidad. <iframe width="100%" height="400" src="https://www.youtube.com/embed/KUSyzxv3wfo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
Con este blog (el primero de una serie), intentaré ilustrar por qué Python y R (junto a GitHub) son las herramientas fundamentales para aquellos que tengan la responsabilidad de dirigir o gerenciar equipos multidisciplinarios. 

# ¿Por qué Python y R para la gestión de proyectos?


Hay varias razones de peso para considerar otras opciones más allá de Excel y de Microsoft Project para gestionar proyectos. A continuación algunas de ellas.


1. La ciencia de datos y la inteligencia artificial no trabajan con herramientas de escritorio como las ofrecidas por Microsoft Office (i.e., Word, Excel, PowerPoint) debido a las llamadas 4Vs: Veracidad, Variabilidad, Volumen, y Velocidad con la que se producen los datos en el mundo real [@Provost2013]. 
2.  Excel es limitado a la hora abrir archivos con datos de gran volumen que están en el orden de los millones de filas o columnas (e.g., Excel no sirve para hacer "Big Data")
3.  Con Microsoft Project no podemos saber los pequeños cambios que ocurrieron en cada uno de los archivos asociados al desarrollo del proyecto. Con Python y R, estos cambios pueden gestionarse de mánera ágil usando una potente herramienta llamada GitHub.
4.  Ni Excel ni Project son útiles para analizar "datos no estructurados" tales como colecciones de documentos, fotos, videos, o audios. En cambio los datos no estructurados son analizables con Python y R de una forma más versátil y ágil. Sobre ello hablaremos más en detalle en un próximo blog sobre [HuggingFace](https://huggingface.co/)
5.  El análisis de datos no estructurados es la base fundamental de herramientas como el aprendizaje automático (conocido en inglés como Machine Learning). 

Parte de lo que veremos en las próximas subsecciones está tomado del libro de texto de Hathaway y Larson [-@Hathaway2021] quienes se inspiraron en la "biblia" de la ciencia de datos escrita por Wickham y Grolemund [-@Wickham2017].

# Extracción, Transformación y Carga de Datos Estructurados

La extracción, transformación y carga de datos estructurados en inglés se abrevia con las siglas "ETL" ("**E**xtraction, **T**ransformation and **L**oading"). ETL es un ejercicio muy típico en tareas de big data sobre las cuales se apoyan rutinariamente aplicaciones de inteligencia artificial tales como el aprendizaje automático. La sintaxis que veremos a continuación nos permite abrir un total de 22 archivos separados por comas (.csv) con datos de vuelos internacionales. Parte de lo que veremos acá está adaptado del blog de Marc Garcia [-@Garcia2022]. Para ello, nos vamos a apoyar en "pandas" la cual es una librería muy conocida dentro de Python.

Esta librería fue diseñada para realizar tareas relacionadas con la lectura, edición, y manipulación de datos estructurados. Por datos estructurados vamos a comprender una tabla en donde las variables que nos interesa analizar se ordenan por columnas, las observaciones de cada variable se ordenan por filas, y los valores de cada observación para cada variable se registran en cada celda, tal como aparece en la siguiente imágen.

![](tidy.png){fig-align="center"}


Además de pandas, vamos a usar otras librerías como "importlib" y "pyarrow" para acortar el tiempo que nos tomará abrir los siguientes datos.

```{python}
import importlib

try:
    import pandas
    import pyarrow
    import pyarrow.csv

    COLUMN_TYPES = {'Origin': pyarrow.dictionary(pyarrow.int32(), pyarrow.string()),
                    'Year': pyarrow.uint16(),
                    'Month': pyarrow.uint8(),
                    'DayofMonth': pyarrow.uint8(),
                    'CRSDepTime': pyarrow.uint16(),
                    'DepTime': pyarrow.uint16()}

    tables = []
    for year in range(1987, 2009):
        tables.append(pyarrow.csv.read_csv(
            f'/home/jcc/dataverse_files/{year}.csv',
            convert_options=pyarrow.csv.ConvertOptions(
                include_columns=COLUMN_TYPES,
                column_types=COLUMN_TYPES)))

    df = pyarrow.concat_tables(tables).to_pandas()
    
except ImportError:
    # Si la librería pyarrow no está instalada, primero asegúrese de instalarla
    print("PyArrow no está instalado, Por favor, instala pyarrow escribiendo 'pip install pyarrow'")
```

Los datos que hemos abierto, tienen la siguiente apariencia

```{python}
df.head(5)
```

Podemos hacer algunos pequeños cambios a la base de datos de la siguiente manera

```{python}
date = pandas.to_datetime(df[['Year', 'Month', 'DayofMonth']].rename(columns={'DayofMonth': 'Day'}))

df['scheduled_dep'] = date + pandas.to_timedelta((df['CRSDepTime'] // 100) * 60 + (df['CRSDepTime'] % 100),
                                                 unit='minutes')
df['actual_dep'] = date + pandas.to_timedelta((df['DepTime'] // 100) * 60 + (df['DepTime'] % 100),
                                            unit='minutes')

del date
df = df[['Origin', 'scheduled_dep', 'actual_dep']]

df['delay'] = (df['actual_dep'] - df['scheduled_dep']).dt.total_seconds() / 60 / 60

df['delay'] = df['delay'].where(df['delay'] > - 2, 24 - df['delay'])
df.tail(5)
```

Luego de esta última sintaxis en Python vemos que la base de datos que hemos abierto tiene un total de 118.914.457 filas y cuatro columnas, aunque originalmente la base de datos tenía un total de seis columnas. Dentro de este mismo blog, podemos incluso combinar códigos de diferentes lenguajes. Por ejemplo, la siguiente sintaxis ya no está en lenguaje de Python, sino en lenguaje de R. 

```{r}
library(reticulate)
# Asegúrate de que el entorno de Python esté cargado correctamente
use_python("~/anaconda3/bin/python3.12")

df <- py$df
hist(df$delay)
```

A continuación te comparto algunas preguntas sugeridas para que profundices en tu aprendizaje sobre la analítica de datos.


# Preguntas de profundización

1. ¿Qué hace la librería importlib y en qué se diferencia de pandas?
2. ¿Exactamente qué se hizo para cambiar la base de datos que originalmente tenía seis variables para tener tan solo cuatro?
3. ¿Cuán relevante sería aprender sobre más herramientas de Big Data para industrias como el comercio y las ventas de consumo masivo?
4. ¿Qué otras opciones podemos seguir para abrir y transformar los datos que hemos usado como ejemplo en este blog (puedes consultar el propio blog de Marc Garcia [-@Garcia2022]).
5. ¿Qué deberíamos cambiar en las sintaxis para incorporar otras variables como la ciudad de destino, la distancia recorrida, el tiempo de cada vuelo, y la hora de llegada a la ciudad de destino?

En un próximo blog, vamos a enfocarnos en la minería de datos y cómo Python y R nos pueden ayudar a analizar estadísticamente datos no estructurados.
